{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **IA Biom√©dica:** Laboratorio PharmaNet: Funciones Avanzadas\n",
    "\n",
    "\n",
    "> ##### Descubrimiento de Farmacos\n",
    "\n",
    "\n",
    "> **Instructores**\n",
    "\n",
    "*  Julio Castellanos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Laboratorio: Uso Avanzado para el Descubrimiento de F√°rmacos\n",
    "\n",
    "En este laboratorio profundizaremos en la interpretaci√≥n de los resultados de inferencia y modificaremos el modelo original para entrenarlo con nuestros propios targets.\n",
    "\n",
    "Al final de esta sesi√≥n, se espera haber logrado lo siguiente:\n",
    "\n",
    "- Entrenar un modelo exclusivamente con datos de targets seleccionados (i.e: prote√≠nas como Thymidine Kinase (KITH) y Fatty Acid-Binding Protein (FABP4)).\n",
    "- Hacer plots de la la funci√≥n de perdida y NAP\n",
    "- Realizar el preprocesamiento de los resultados de inferencia para mejorar su interpretaci√≥n.\n",
    "- Validar los datos en UniProt y hacer interpretaci√≥n estructural con AlphaFold y Boltz-1 en Fastfold.\n",
    "\n",
    "\n",
    "### Referencias:\n",
    "\n",
    "- PharmaNet: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0241728\n",
    "- KITH: https://www.uniprot.org/uniprotkb/P04183/entry#sequences\n",
    "- FABP4: https://www.uniprot.org/uniprotkb/P04183/entry#sequences\n",
    "- Alphafold: https://www.nature.com/articles/s41586-024-07487-w\n",
    "- Boltz-1: https://www.biorxiv.org/content/10.1101/2024.11.19.624167v1\n",
    "- Fastfold: https://www.fastfold.ai\n",
    "\n",
    "### Modelo\n",
    "\n",
    "<p align=\"center\"><img src=\"Overview.png\" /></p>\n",
    "\n",
    "\n",
    "### Top Targets \n",
    "\n",
    "<p align=\"center\"><img src=\"data/lab/top_targets.png\" /></p>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliocesar/miniforge3/envs/PharmaNet/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.9.0.post2\n"
     ]
    }
   ],
   "source": [
    "# ## 1. Importaciones Principales\n",
    "\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Modulos propios (aseg√∫rate de tener estas rutas en tu proyecto)\n",
    "from data.data_loader import SMILESDataset\n",
    "from utils.metrics import pltauc, norm_ap_optimized\n",
    "from models.Model import Model\n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n de Par√°metros y Semilla\n",
    "\n",
    "En esta secci√≥n, definimos los par√°metros necesarios para la ejecuci√≥n:\n",
    "- GPU o CPU\n",
    "- Fijaci√≥n de semilla aleatoria para reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo seleccionado: cpu\n"
     ]
    }
   ],
   "source": [
    "# 2. Configuraci√≥n de Par√°metros y Semilla\n",
    "\n",
    "seed = 6766\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Configurar GPU (si est√° disponible)\n",
    "ngpu = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(ngpu)\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "print(\"Dispositivo seleccionado:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lectura de Datos\n",
    "\n",
    "Tomamos los datos y filtramos las filas para mantener solo los registros con Target ‚Äúkith‚Äù o ‚Äúfabp4‚Äù, convierte esos valores a etiquetas num√©ricas (1 para ‚Äúkith‚Äù y 0 para ‚Äúfabp4‚Äù), ordena los datos por la etiqueta y finalmente guarda los resultados filtrados y ordenados en archivos CSV separados para entrenamiento y prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for train: 792\n",
      "Data for test : 274\n"
     ]
    }
   ],
   "source": [
    "# 3. Lectura de Datos\n",
    "train_path = \"data/datasets/DUDE/\"\n",
    "A = pd.read_csv(train_path + 'Smiles_1.csv')\n",
    "B = pd.read_csv(train_path + 'Smiles_2.csv')\n",
    "C = pd.read_csv(train_path + 'Smiles_3.csv')\n",
    "D = pd.read_csv(train_path + 'Smiles_4.csv')\n",
    "\n",
    "data_train = pd.concat([A, B, C], ignore_index=True)\n",
    "data_test = D\n",
    "\n",
    "# Filtrado de los datos para conservar solo las filas con Targets deseados\n",
    "targets_to_keep = ['kith', 'fabp4', 'cxcr4', 'comt', 'hmdh', 'akt1', 'ada17']\n",
    "label_mapping = {'fabp4': 0, 'kith': 1, 'cxcr4': 2, 'comt': 3, 'hmdh': 4, 'akt1': 5, 'ada17': 6}\n",
    "\n",
    "# Para data_train: filtrar, seleccionar las columnas deseadas, actualizar la columna Label y ordenar\n",
    "filtered_train = data_train[data_train['Target'].isin(targets_to_keep)][['Smiles', 'Target', 'Label']].copy()\n",
    "filtered_train['Label'] = filtered_train['Target'].map(label_mapping)\n",
    "filtered_train = filtered_train.sort_values(by='Label')\n",
    "filtered_train.to_csv(train_path + \"LAB_TRAIN/train_data.csv\", index=False)\n",
    "\n",
    "# Para data_test: filtrar, seleccionar las columnas deseadas, actualizar la columna Label y ordenar\n",
    "filtered_test = data_test[data_test['Target'].isin(targets_to_keep)][['Smiles', 'Target', 'Label']].copy()\n",
    "filtered_test['Label'] = filtered_test['Target'].map(label_mapping)\n",
    "filtered_test = filtered_test.sort_values(by='Label')\n",
    "filtered_test.to_csv(train_path + \"LAB_TRAIN/test_data.csv\", index=False)\n",
    "\n",
    "\n",
    "# Filtrar y guardar datos para entrenamiento\n",
    "filtered_train = data_train[data_train['Target'].isin(targets_to_keep)][['Smiles', 'Target', 'Label']].copy()\n",
    "filtered_train['Label'] = filtered_train['Target'].map(label_mapping)\n",
    "filtered_train = filtered_train.sort_values(by='Label')\n",
    "filtered_train = filtered_train.reset_index(drop=True)  # Reset index\n",
    "filtered_train.to_csv(os.path.join(train_path, \"LAB_TRAIN/train_data.csv\"), index=False)\n",
    "\n",
    "# Filtrar y guardar datos para test\n",
    "filtered_test = data_test[data_test['Target'].isin(targets_to_keep)][['Smiles', 'Target', 'Label']].copy()\n",
    "filtered_test['Label'] = filtered_test['Target'].map(label_mapping)\n",
    "filtered_test = filtered_test.sort_values(by='Label')\n",
    "filtered_test = filtered_test.reset_index(drop=True)  # Reset index\n",
    "filtered_test.to_csv(os.path.join(train_path, \"LAB_TRAIN/test_data.csv\"), index=False)\n",
    "\n",
    "# Luego, al asignar los datos para el dataset:\n",
    "data_train = filtered_train\n",
    "data_test = filtered_test\n",
    "\n",
    "print('Data for train:', len(data_train['Smiles']))\n",
    "print('Data for test :', len(data_test['Smiles']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparaci√≥n del Vocabulario (charset) y Definici√≥n de Longitud M√°xima\n",
    "Extraemos un conjunto de caracteres √∫nicos usados en los SMILES y calculamos la longitud m√°xima de la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caracteres √∫nicos: {'B', '=', '7', '8', '1', '@', 'S', 'O', 'o', '-', '4', 'I', '(', 'l', 'P', '/', 'r', '\\\\', 's', '2', 'N', '[', 'C', '6', ')', '5', 'n', 'c', 'H', ']', 'F', '#', '3'}\n",
      "Total de caracteres: 33\n",
      "Max. longitud de SMILE: 112\n"
     ]
    }
   ],
   "source": [
    "# 4. Vocabulario y Embedding\n",
    "charset = set(\"\".join(list(data_train.Smiles)) + \"\".join(list(data_test.Smiles)))\n",
    "vocab_size = len(charset)\n",
    "char_to_int = dict((c, i) for i, c in enumerate(charset))\n",
    "\n",
    "embed_tr = max([len(smile) for smile in data_train.Smiles])\n",
    "embed_te = max([len(smile) for smile in data_test.Smiles])\n",
    "embed = max(embed_tr, embed_te)\n",
    "\n",
    "print('Caracteres √∫nicos:', str(charset))\n",
    "print('Total de caracteres:', vocab_size)\n",
    "print('Max. longitud de SMILE:', embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construcci√≥n del Modelo (PharmaNet)\n",
    "\n",
    "Se instancia la clase `Model(...)` que define la arquitectura Conv + RNN.  \n",
    "Luego se imprime el modelo para revisar su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Model(\n",
      "  (module_mol): ModuleList(\n",
      "    (0): Conv2d(33, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(3, 2))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): GRU(128, 256, num_layers=10, batch_first=True, bidirectional=True)\n",
      "    (5): Upsample(size=64, mode=nearest)\n",
      "  )\n",
      "  (fc1): Linear(in_features=512, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Construcci√≥n del Modelo\n",
    "num_classes = max(data_train.Label) + 1\n",
    "print(num_classes)\n",
    "hidden_size = 256\n",
    "kernel_size = 5\n",
    "\n",
    "bidireccional = True\n",
    "num_layers = 10\n",
    "\n",
    "net = Model(vocab_size,\n",
    "            num_classes,\n",
    "            hidden_size,\n",
    "            bidireccional,\n",
    "            num_layers,\n",
    "            kernel_size\n",
    "            ).to(device)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Definici√≥n del Optimizador y la Funci√≥n de P√©rdida\n",
    "\n",
    "Usamos Adam como optimizador y BCELoss (aplicando `sigmoid` en cada clase).  \n",
    "El scheduler reduce el learning rate si no hay mejoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Optimizador y P√©rdida\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=7)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creaci√≥n de los DataLoaders (Train/Test)\n",
    "\n",
    "Se definen:\n",
    "- El dataset con la clase `SMILESDataset`.\n",
    "- DataLoader con sampler balanceado o shuffle, seg√∫n los argumentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader y test loader creados.\n"
     ]
    }
   ],
   "source": [
    "# 7. DataLoaders\n",
    "\n",
    "neighbours = 0\n",
    "padding = False\n",
    "\n",
    "batch_size = 128\n",
    "workers = 5\n",
    "\n",
    "train_datasets = SMILESDataset(data_train, vocab_size, char_to_int, embed, neighbours, padding)\n",
    "test_datasets  = SMILESDataset(data_test, vocab_size, char_to_int, embed, neighbours, padding)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_datasets, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=workers)\n",
    "test_loader  = DataLoader(test_datasets,  batch_size=batch_size, drop_last=True, shuffle=True, num_workers=workers)\n",
    "\n",
    "print(\"Train loader y test loader creados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Funciones de Entrenamiento y Evaluaci√≥n\n",
    "\n",
    "- **train(epoch)**: Modo `train()`, calcula p√©rdida y backprop.\n",
    "- **evaluate(epoch)**: Modo `eval()`, sin backprop, y se calculan m√©tricas finales (NAP, AUC, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    # Coloca el modelo en modo entrenamiento (permite dropout, actualiza gradientes)\n",
    "    net.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    lab = np.array([])                       # Para almacenar etiquetas reales\n",
    "    maps = np.empty((0, num_classes))        # Para almacenar predicciones (probabilidades)\n",
    "\n",
    "    # Iteramos sobre cada batch (inputs, _, labels) proveniente de train_loader\n",
    "    for inputs, _, labels in tqdm(train_loader):\n",
    "        # Convertimos a tensores en float32 y los movemos al dispositivo (CPU/GPU)\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Reiniciamos gradientes antes de cada batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Activamos el c√°lculo de gradientes\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Forward pass: obtenemos outputs de la red\n",
    "            outputs = net(inputs)\n",
    "            loss = 0\n",
    "\n",
    "            # Para cada clase, calculamos el aporte a la p√©rdida usando BCELoss\n",
    "            for i in range(num_classes):\n",
    "                labe = labels.clone()\n",
    "                labe[labels == i] = 1\n",
    "                labe[labels != i] = 0\n",
    "                loss += criterion(F.sigmoid(outputs[:, i].float()), labe.float().to(device))\n",
    "\n",
    "            # Convertimos outputs a numpy para calcular m√©tricas (softmax de outputs)\n",
    "            outputs_array = F.softmax(outputs.clone().detach().cpu(), dim=1).numpy()\n",
    "            maps = np.append(maps, outputs_array, axis=0)\n",
    "\n",
    "            # Guardamos las etiquetas en un array para m√©tricas\n",
    "            labels_array = labels.clone().cpu().detach().numpy()\n",
    "            lab = np.append(lab, labels_array)\n",
    "\n",
    "            # Obtenemos la predicci√≥n (clase con mayor probabilidad)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Backprop: calculamos gradientes y actualizamos pesos\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Acumulamos p√©rdida total del batch\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Calculamos aciertos para accuracy\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # P√©rdida media en la √©poca\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    # Precisi√≥n en la √©poca\n",
    "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "    # C√°lculo de m√©tricas: NAP y AUC (incluye macro y micro)\n",
    "    epoch_nap, epoch_f = norm_ap_optimized(maps, lab, num_classes)\n",
    "    epoch_auc = pltauc(maps, lab, num_classes)\n",
    "\n",
    "    # Imprimimos m√©tricas clave\n",
    "    print('Phase Train, Loss: {:.4f} Acc: {:.4f} NAP: {:.4f} F-measure: {:.4f} AUC: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc, epoch_nap[-1], epoch_f[-1], epoch_auc[\"micro\"]))\n",
    "\n",
    "    # Retornamos valores relevantes para graficar o almacenar\n",
    "    return epoch_loss, epoch_acc, epoch_nap[-1], epoch_f[-1], epoch_auc[\"micro\"]\n",
    "\n",
    "\n",
    "def evaluate(epoch):\n",
    "    net.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = torch.tensor(0.0, device=device)  # Changed initialization\n",
    "\n",
    "    lab = np.array([])\n",
    "    maps = np.empty((0, num_classes))\n",
    "\n",
    "    # Iterate over validation data\n",
    "    for inputs, _, labels in test_loader:\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = net(inputs)\n",
    "            loss = 0\n",
    "            for i in range(num_classes):\n",
    "                labe = labels.clone()\n",
    "                labe[labels == i] = 1\n",
    "                labe[labels != i] = 0\n",
    "                loss += criterion(F.sigmoid(outputs[:, i].float()), labe.float().to(device))\n",
    "\n",
    "            outputs_array = F.softmax(outputs.clone().detach().cpu(), dim=1).numpy()\n",
    "            maps = np.append(maps, outputs_array, axis=0)\n",
    "            labels_array = labels.clone().cpu().detach().numpy()\n",
    "            lab = np.append(lab, labels_array)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(test_loader.dataset)\n",
    "    \n",
    "    # Update scheduler using the aggregated loss\n",
    "    scheduler.step(epoch_loss)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(param_group['lr'])\n",
    "        \n",
    "    epoch_acc = running_corrects.double() / len(test_loader.dataset)\n",
    "    epoch_nap, epoch_f = norm_ap_optimized(maps, lab, num_classes)                \n",
    "    epoch_auc = pltauc(maps, lab, num_classes)\n",
    "    \n",
    "    predictions = [maps, lab]\n",
    "    \n",
    "    print('Phase Validation, Loss: {:.4f} Acc: {:.4f} NAP: {:.4f} F-measure: {:.4f} auc:  {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc, epoch_nap[-1], epoch_f[-1], epoch_auc[\"micro\"]))\n",
    "    \n",
    "    return epoch_loss, epoch_acc, epoch_nap[-1], epoch_f[-1], epoch_auc[\"micro\"], predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plots de Metricas \n",
    "\n",
    "Graficamos la metricas para hacer seguimiento al progreso en tiempo de ejecucion del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_loss(d_losses, g_losses, save_dir, num_epoch, save=True, show=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0,num_epoch)\n",
    "    ax.set_ylim(0, max(np.max(g_losses), np.max(d_losses))*1.1)\n",
    "    plt.xlabel('Epoch {0}'.format(num_epoch))\n",
    "    plt.ylabel('Loss values')\n",
    "    plt.plot(d_losses, label='Train')\n",
    "    plt.plot(g_losses, label='Test')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_fn = save_dir + 'losses_{:d}'.format(num_epoch) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "def plot_nap(d_losses, g_losses, f_losses, td_losses, tg_losses, tf_losses, save_dir, num_epoch, save=True, show=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0,num_epoch)\n",
    "    ax.set_ylim(0, max(np.max(g_losses), np.max(d_losses), np.max(f_losses))*1.1)\n",
    "    plt.xlabel('Epoch {0}'.format(num_epoch))\n",
    "    plt.ylabel('metric values')\n",
    "    plt.plot(d_losses, label='Test nap')\n",
    "    plt.plot(g_losses, label='Test AUC')\n",
    "    plt.plot(f_losses, label='Test accuracy')\n",
    "    plt.plot(td_losses, label='Train nap')\n",
    "    plt.plot(tg_losses, label='Train AUC')\n",
    "    plt.plot(tf_losses, label='Train accuracy')\n",
    "    plt.legend()\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_fn = save_dir + 'nap{:d}'.format(num_epoch) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Bucle Principal de Entrenamiento\n",
    "\n",
    "Controla el n√∫mero de √©pocas, guarda el mejor modelo seg√∫n la m√©trica (NAP, por ejemplo) y produce gr√°ficas de evoluci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0\n",
    "num_epochs = 2\n",
    "\n",
    "results_dir = './PharmaNet'\n",
    "results_path = 'Lab_Advanced'\n",
    "\n",
    "# Historial para gr√°ficas\n",
    "train_losses_history, val_losses_history = [], []\n",
    "train_acc_history, val_acc_history = [], []\n",
    "train_nap_history, val_nap_history = [], []\n",
    "train_f_history, val_f_history = [], []\n",
    "train_auc_history, val_auc_history = [], []\n",
    "\n",
    "best_model_wts = copy.deepcopy(net.state_dict())\n",
    "saving_path_models = os.path.join(results_dir, results_path, 'model_1'+'/')\n",
    "if not os.path.exists(saving_path_models):\n",
    "    os.makedirs(saving_path_models, 0o777)\n",
    "\n",
    "def main():\n",
    "    since = time.time()\n",
    "    best_NAP = 0\n",
    "    best_prediction = None\n",
    "\n",
    "    for epoch in range(initial_epoch, num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Entrenar\n",
    "        train_loss, train_acc, train_nap, train_f, train_auc = train(epoch)\n",
    "        train_losses_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "        train_auc_history.append(train_auc)\n",
    "        train_nap_history.append(train_nap)\n",
    "        train_f_history.append(train_f)\n",
    "\n",
    "        # Validar\n",
    "        val_loss, val_acc, val_nap, val_f, val_auc, prediction_nap = evaluate(epoch)\n",
    "        val_losses_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "        val_auc_history.append(val_auc)\n",
    "        val_nap_history.append(val_nap)\n",
    "        val_f_history.append(val_f)\n",
    "\n",
    "        # Guardar el mejor modelo si la m√©trica (NAP) mejora\n",
    "        if val_nap > best_NAP:\n",
    "            best_NAP = val_nap\n",
    "            best_prediction = prediction_nap\n",
    "            best_model_wts = copy.deepcopy(net.state_dict())\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_losses_history\n",
    "            }, os.path.join(saving_path_models, 'Checkpoint.pth'))\n",
    "            \n",
    "        # Graficar loss y nap\n",
    "        \n",
    "        plot_loss(train_losses_history,\n",
    "                val_losses_history, \n",
    "                saving_path_models,\n",
    "                num_epochs,\n",
    "                show=True)\n",
    "        plot_nap(val_nap_history, val_auc_history, \n",
    "                torch.tensor(val_acc_history).cpu().tolist(),\n",
    "                train_nap_history, train_auc_history, \n",
    "                torch.tensor(train_acc_history).cpu().tolist(),\n",
    "                saving_path_models,\n",
    "                num_epochs,\n",
    "                show=True)   \n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val NAP: {:4f}'.format(best_NAP))\n",
    "    print('Best val ACC: {:4f}'.format(max(val_acc_history)))\n",
    "    print('Best val AUC: {:4f}'.format(max(val_auc_history)))\n",
    "    print('Best val Fscore: {:4f}'.format(max(val_f_history)))\n",
    "\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    torch.save({\n",
    "        'model': net.state_dict(),\n",
    "        'optimize': optimizer.state_dict(),\n",
    "        'prediction': best_prediction,\n",
    "        'charset': char_to_int,\n",
    "        'embed': embed\n",
    "    }, os.path.join(saving_path_models, 'model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ejecucion de Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:02<00:00, 30.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase Train, Loss: 2.9944 Acc: 0.3333 NAP: 0.2771 F-measure: 0.2747 AUC: 0.4942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n",
      "Phase Validation, Loss: 2.2323 Acc: 0.4161 NAP: 0.3827 F-measure: 0.4133 auc:  0.5358\n",
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:15<00:00, 32.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase Train, Loss: 2.2716 Acc: 0.4381 NAP: 0.3012 F-measure: 0.2979 AUC: 0.5140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n",
      "Phase Validation, Loss: 2.2336 Acc: 0.4161 NAP: 0.2783 F-measure: 0.3258 auc:  0.5025\n",
      "Training complete in 7m 18s\n",
      "Best val NAP: 0.382696\n",
      "Best val ACC: 0.416058\n",
      "Best val AUC: 0.535819\n",
      "Best val Fscore: 0.413256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_train = True\n",
    "\n",
    "if run_train:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inferencia usando Torch Save\n",
    "\n",
    "**Preparaci√≥n de predicciones**\n",
    "Tras la inferencia, se acumulan las salidas softmax (all_outputs), las etiquetas reales (all_labels) y se calculan las predicciones con np.argmax para determinar la etiqueta con mayor probabilidad.\n",
    "\n",
    "**Guardado con torch.save**\n",
    "Se crea un diccionario llamado predictions que contiene las salidas, las etiquetas reales y las etiquetas predichas. Luego, este diccionario se guarda en un archivo llamado test_predictions.pth en la ruta saving_path_models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde: ./PharmaNet/Lab_Advanced/model_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:35<00:00, 11.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RESULTADOS DE INFERENCIA]\n",
      "  NAP: 0.2722\n",
      "  AP (micro): 0.1582\n",
      "  AUC (micro): 0.5937\n",
      "  Accuracy: 0.4453\n",
      "\n",
      "Final predictions saved to: ./PharmaNet/Lab_Advanced/model_1/test_predictions.pth\n",
      "{'NAP': 0.2721622513264338, 'AP_micro': 0.15823164987230323, 'AUC_micro': 0.5937184595146274, 'Accuracy': 0.44525547445255476}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.data_loader import SMILESDataset\n",
    "from utils.metrics import pltmap, pltauc, norm_ap\n",
    "from models.Model import Model\n",
    "\n",
    "def inference_example(\n",
    "    data_test,           # DataFrame con SMILES y etiquetas\n",
    "    saving_path_models,  # Ruta donde est√° el modelo entrenado (model.pth)\n",
    "    device,              # \"cuda\" o \"cpu\"\n",
    "    num_classes,         # Cantidad de clases/targets esperadas en el modelo\n",
    "    hidden_size,         # Tama√±o hidden para la red\n",
    "    bidireccional,       # Boolean, si la RNN es bidireccional\n",
    "    num_layers,          # N√∫mero de capas en la RNN\n",
    "    kernel_size,         # Tama√±o kernel para convoluci√≥n\n",
    "    neighbours,          # Par√°metro extra para SMILESDataset\n",
    "    padding,             # Padding usado en SMILESDataset\n",
    "    batch_size,          # Tama√±o de batch para DataLoader\n",
    "    workers              # Num. de workers para DataLoader\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejemplo simplificado de inferencia con PharmaNet.\n",
    "    - data_test: DataFrame con columnas ['Smiles','Label']\n",
    "    - Se asume que 'model.pth' contiene ['model','charset','embed'].\n",
    "    \"\"\"\n",
    "    print(\"Cargando modelo desde:\", saving_path_models)\n",
    "    model_path = os.path.join(saving_path_models, 'model.pth')\n",
    "    trained_model = torch.load(model_path, map_location=device)\n",
    "\n",
    "    # Reconstruir diccionario de caracteres y embed\n",
    "    char_to_int_inf = trained_model['charset']\n",
    "    embed_inf       = trained_model['embed']\n",
    "    vocab_size_inf  = len(char_to_int_inf)\n",
    "\n",
    "    # Reconstruir el modelo con los mismos hiperpar√°metros\n",
    "    model_infer = Model(\n",
    "        vocab_size_inf,\n",
    "        num_classes,\n",
    "        hidden_size,\n",
    "        bidireccional,\n",
    "        num_layers,\n",
    "        kernel_size\n",
    "    ).to(device)\n",
    "\n",
    "    model_infer.load_state_dict(trained_model['model'])\n",
    "    model_infer.eval()\n",
    "\n",
    "    # Crear dataset de prueba\n",
    "    test_dataset_inf = SMILESDataset(\n",
    "        data_test,\n",
    "        vocab_size_inf,\n",
    "        char_to_int_inf,\n",
    "        embed_inf,\n",
    "        neighbours,\n",
    "        padding\n",
    "    )\n",
    "\n",
    "    test_loader_inf = DataLoader(\n",
    "        test_dataset_inf,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=workers\n",
    "    )\n",
    "\n",
    "    all_outputs = np.empty((0, num_classes))\n",
    "    all_labels  = np.array([])\n",
    "\n",
    "    # Bucle de inferencia\n",
    "    for inputs, _, labels in tqdm(test_loader_inf, desc=\"Inferencia\"):\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_infer(inputs)    # (batch_size, num_classes)\n",
    "            out_soft = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "            all_outputs = np.append(all_outputs, out_soft, axis=0)\n",
    "\n",
    "            # Guardamos las etiquetas reales para m√©tricas\n",
    "            labels_np = labels.cpu().numpy()\n",
    "            all_labels = np.append(all_labels, labels_np)\n",
    "\n",
    "    # ===================\n",
    "    # C√°lculo de m√©tricas\n",
    "    # ===================\n",
    "    unique_classes = np.unique(all_labels)\n",
    "    if len(unique_classes) < num_classes:\n",
    "        print(f\"Warning: Se esperaban {num_classes} clases, pero solo se encontraron {len(unique_classes)} en el test.\")\n",
    "    \n",
    "    # Intentamos calcular cada m√©trica y, si falla, asignamos un valor por defecto.\n",
    "    try:\n",
    "        epoch_nap, epoch_f = norm_ap(all_outputs, all_labels, num_classes=num_classes)\n",
    "    except Exception as e:\n",
    "        print(\"Error al calcular NAP:\", e)\n",
    "        epoch_nap, epoch_f = [0.0], [0.0]\n",
    "    \n",
    "    try:\n",
    "        epoch_ap, _ = pltmap(all_outputs, all_labels, num_classes=num_classes)\n",
    "    except Exception as e:\n",
    "        print(\"Error al calcular MAP:\", e)\n",
    "        epoch_ap = {\"micro\": 0.0}\n",
    "    \n",
    "    try:\n",
    "        # Si hay menos de dos clases presentes, evitamos el c√°lculo de AUC\n",
    "        if len(unique_classes) < 2:\n",
    "            print(\"Solo hay una clase en los datos. No se puede calcular AUC.\")\n",
    "            epoch_auc = {\"micro\": 0.0, \"macro\": 0.0}\n",
    "        else:\n",
    "            epoch_auc = pltauc(all_outputs, all_labels, num_classes=num_classes)\n",
    "    except Exception as e:\n",
    "        print(\"Error al calcular AUC:\", e)\n",
    "        epoch_auc = {\"micro\": 0.0, \"macro\": 0.0}\n",
    "\n",
    "    # Accuracy\n",
    "    preds = np.argmax(all_outputs, axis=1)\n",
    "    running_corrects = np.sum(preds == all_labels)\n",
    "    epoch_acc = running_corrects / len(all_labels) if len(all_labels) > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n[RESULTADOS DE INFERENCIA]\")\n",
    "    print(f\"  NAP: {epoch_nap[-1]:.4f}\")\n",
    "    print(f\"  AP (micro): {epoch_ap['micro']:.4f}\")\n",
    "    print(f\"  AUC (micro): {epoch_auc['micro']:.4f}\")\n",
    "    print(f\"  Accuracy: {epoch_acc:.4f}\\n\")\n",
    "\n",
    "    # Preparar un diccionario con las predicciones finales\n",
    "    predictions = {\n",
    "        'outputs': all_outputs,           # Salidas softmax para cada muestra\n",
    "        'labels': all_labels,             # Etiquetas reales\n",
    "        'predicted_labels': preds         # Etiquetas predichas (argmax)\n",
    "    }\n",
    "\n",
    "    # Guardar las predicciones finales usando torch.save\n",
    "    save_path = os.path.join(saving_path_models, 'test_predictions.pth')\n",
    "    torch.save({\n",
    "        'prediction': predictions\n",
    "    }, save_path)\n",
    "    print(\"Final predictions saved to:\", save_path)\n",
    "\n",
    "    return {\n",
    "        \"NAP\": epoch_nap[-1],\n",
    "        \"AP_micro\": epoch_ap[\"micro\"],\n",
    "        \"AUC_micro\": epoch_auc[\"micro\"],\n",
    "        \"Accuracy\": epoch_acc\n",
    "    }\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# Se asume que ya tienes definidos data_test, saving_path_models, device, num_classes,\n",
    "# hidden_size, bidireccional, num_layers, kernel_size, neighbours, padding, batch_size y workers.\n",
    "resultados = inference_example(\n",
    "     data_test=data_test,\n",
    "     saving_path_models=saving_path_models,\n",
    "     device=\"cpu\",\n",
    "     num_classes=num_classes,\n",
    "     hidden_size=hidden_size,\n",
    "     bidireccional=bidireccional,\n",
    "     num_layers=num_layers,\n",
    "     kernel_size=kernel_size,\n",
    "     neighbours=neighbours,\n",
    "     padding=padding,\n",
    "     batch_size=batch_size,\n",
    "     workers=workers\n",
    ")\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia a targets especificos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Target</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nc1ncnc2n(cnc12)[C@@H]1O[C@H](COP([O-])(=O)OP(...</td>\n",
       "      <td>kith</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[H][C@]12CC[C@]3(C)[C@@H](O)CC[C@@]3([H])[C@]1...</td>\n",
       "      <td>comt</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Smiles Target  Label\n",
       "0  Nc1ncnc2n(cnc12)[C@@H]1O[C@H](COP([O-])(=O)OP(...   kith      1\n",
       "1  [H][C@]12CC[C@]3(C)[C@@H](O)CC[C@@]3([H])[C@]1...   comt      3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_validation = pd.read_csv('data/datasets/DUDE/LAB_TRAIN/inference_validation.csv')\n",
    "data_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Proteina 1**\n",
    "Target: Thymidine Kinase (KITH)\n",
    "\n",
    "<p align=\"center\"><img style=\"width: 350px\" src=\"https://raw.githubusercontent.com/juliocesar-io/PharmaNet/refs/heads/main/data/lab/KITH.png\" /></p>\n",
    "\n",
    "\n",
    "Fuente: Resultado prediccion en https://cloud.fastfold.ai con Boltz-1\n",
    "\n",
    "Sequencia: \n",
    "\n",
    "```\n",
    "MSCINLPTVLPGSPSKTRGQIQVILGPMFSGKSTELMRRVRRFQIAQYKCLVIKYAKDTRYSSSFCTHDRNTMEALPACLLRDVAQEALGVAVIGIDEGQFFPDIVEFCEAMANAGKTVIVAALDGTFQRKPFGAILNLVPLAESVVKLTAVCMECFREAAYTKRLGTEKEVEVIGGADKYHSVCRLCYFKKASGQPAGPDNKENCPVPGKPGEAVAARKLFAPQQILQCSPAN\n",
    "```\n",
    "\n",
    "\n",
    "**Molecula (SMILES) 1**\n",
    "\n",
    "ATP\n",
    "\n",
    "<p align=\"center\"><img style=\"width: 150px\" src=\"https://raw.githubusercontent.com/juliocesar-io/PharmaNet/refs/heads/main/data/lab/atp.png\" /></p>\n",
    "\n",
    "Secuencia:\n",
    "```\n",
    "Nc1ncnc2n(cnc12)[C@@H]1O[C@H](COP([O-])(=O)OP([O-])(=O)OP([O-])([O-])=O)[C@@H](O)[C@H]1O\n",
    "```\n",
    "\n",
    "Ref: https://www.uniprot.org/uniprotkb/P04183/entry#sequences\n",
    "\n",
    "\n",
    "**Proteina 2**\n",
    "Target: Catechol O-methyltransferase (COMT)\n",
    "\n",
    "<p align=\"center\"><img style=\"width: 350px\" src=\"https://raw.githubusercontent.com/juliocesar-io/PharmaNet/refs/heads/main/data/lab/COMT.png\" /></p>\n",
    "\n",
    "\n",
    "Fuente: Resultado prediccion en https://cloud.fastfold.ai con Boltz-1\n",
    "\n",
    "Sequencia: \n",
    "\n",
    "```\n",
    "MGDTKEQRILNHVLQHAEPGNAQSVLEAIDTYCEQKEWAMNVGDKKGKIVGAVIQEHQPFVLLELGAYCGYSAVRMARLLSPGARLITIEINPDCAAITQRMVDFAGVKDKVTLVVGASQDIIPQLKKKYDVDTLDMVFLDHWKDRYLPDTLLLEECGLLRRGTVLLADNVICPGAPDFLAHVRGSSCFECTHYQSFLEYREVVDGLEKAIYKGPGSEAGP\n",
    "```\n",
    "\n",
    "**Molecula (SMILES) 2**\n",
    "\n",
    "S-adenosyl-L-methionine zwitterion\n",
    "\n",
    "<p align=\"center\"><img style=\"width: 150px\" src=\"https://raw.githubusercontent.com/juliocesar-io/PharmaNet/refs/heads/main/data/lab/S-adenosyl-L-methionine.png\" /></p>\n",
    "\n",
    "Secuencia:\n",
    "```\n",
    "C[S+](CC[C@H]([NH3+])C([O-])=O)C[C@H]1O[C@H]([C@H](O)[C@@H]1O)n1cnc2c(N)ncnc12\n",
    "```\n",
    "\n",
    "Ref: https://www.uniprot.org/uniprotkb/P04183/entry#sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde: ./PharmaNet/Lab_Advanced/model_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Se esperaban 7 clases, pero solo se encontraron 2 en el test.\n",
      "Error al calcular AUC: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "[RESULTADOS DE INFERENCIA]\n",
      "  NAP: nan\n",
      "  AP (micro): 0.2143\n",
      "  AUC (micro): 0.0000\n",
      "  Accuracy: 0.0000\n",
      "\n",
      "Final predictions saved to: ./PharmaNet/Lab_Advanced/model_1/test_predictions.pth\n",
      "{'NAP': nan, 'AP_micro': 0.21428571428571427, 'AUC_micro': 0.0, 'Accuracy': 0.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_validation = pd.read_csv('data/datasets/DUDE/LAB_TRAIN/inference_validation.csv')\n",
    "\n",
    "resultados = inference_example(\n",
    "     data_test=data_validation,\n",
    "     saving_path_models=saving_path_models,\n",
    "     device=\"cpu\",\n",
    "     num_classes=num_classes,\n",
    "     hidden_size=hidden_size,\n",
    "     bidireccional=bidireccional,\n",
    "     num_layers=num_layers,\n",
    "     kernel_size=kernel_size,\n",
    "     neighbours=neighbours,\n",
    "     padding=padding,\n",
    "     batch_size=batch_size,\n",
    "     workers=workers\n",
    ")\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Interpretacion Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability array shape: (2, 7)\n",
      "Sample # 0\n",
      "  Labeled ProbDist %: {'fabp4(0)': '4.23546%', 'kith(1)': '4.77052%', 'cxcr4(2)': '3.73392%', 'comt(3)': '3.05080%', 'hmdh(4)': '6.04587%', 'akt1(5)': '20.28843%', 'ada17(6)': '57.87501%'}\n",
      "  Highest Probability: {'ada17(6)': '57.9%'}\n",
      "\n",
      "Sample # 1\n",
      "  Labeled ProbDist %: {'fabp4(0)': '19.31451%', 'kith(1)': '17.74733%', 'cxcr4(2)': '17.84510%', 'comt(3)': '22.97921%', 'hmdh(4)': '12.81689%', 'akt1(5)': '5.96058%', 'ada17(6)': '3.33638%'}\n",
      "  Highest Probability: {'comt(3)': '23.0%'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TIP: You can inspect a pth file with https://netron.app/ online.\n",
    "\n",
    "\n",
    "def plot_predictions(maps, label_mapping, smiles=None, lab=None, path=None):\n",
    "    \"\"\"\n",
    "    Plot the probability distribution for each sample.\n",
    "    The highest probability is highlighted with a dotted outline and a horizontal dotted line.\n",
    "    \n",
    "    Parameters:\n",
    "      maps: np.array of model probabilities (N x num_classes)\n",
    "      label_mapping: dict mapping labels to indices\n",
    "      smiles (optional): np.array of SMILES strings for each sample\n",
    "      lab (optional): np.array of ground-truth labels for each sample\n",
    "    \"\"\"\n",
    "    num_samples = len(maps)\n",
    "    # Create a list of label names in the format \"label(index)\"\n",
    "    labels = [f\"{label}({idx})\" for label, idx in label_mapping.items()]\n",
    "    x = np.arange(len(labels))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        probs = maps[i]\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        \n",
    "        # Plot the bars (convert probabilities to percentage)\n",
    "        bars = plt.bar(x, probs * 100, align='center', alpha=0.7)\n",
    "        \n",
    "        # Determine the maximum probability index and value\n",
    "        max_index = probs.argmax()\n",
    "        max_prob_percent = probs[max_index] * 100\n",
    "        \n",
    "        # Highlight the highest probability bar:\n",
    "        # - Set a red dotted outline on the highest bar\n",
    "        bars[max_index].set_edgecolor('red')\n",
    "        bars[max_index].set_linewidth(2)\n",
    "        bars[max_index].set_linestyle('--')\n",
    "        \n",
    "        # Draw a horizontal dotted line at the highest probability level\n",
    "        plt.axhline(y=max_prob_percent, color='red', linestyle='--', linewidth=1)\n",
    "        \n",
    "        # Annotate the highest probability value above the corresponding bar\n",
    "        plt.text(x[max_index], max_prob_percent + 1, f\"{max_prob_percent:.1f}%\", \n",
    "                 ha='center', color='red', fontweight='bold')\n",
    "        \n",
    "        # Set the x-axis tick labels\n",
    "        plt.xticks(x, labels, rotation=45)\n",
    "        plt.xlabel('Labels')\n",
    "        plt.ylabel('Probability (%)')\n",
    "        \n",
    "        # Use SMILES as title if provided; otherwise, just sample number\n",
    "        title_str = f\"Sample #{i}\"\n",
    "        if smiles is not None:\n",
    "            title_str += f\": {smiles[i]}\"\n",
    "        plt.title(title_str)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig(f\"{path}/prediction_{i}.png\")\n",
    "\n",
    "def display_predictions(maps, label_mapping, smiles=None, lab=None):\n",
    "    \"\"\"\n",
    "    Display the probability distribution for each sample.\n",
    "    \n",
    "    Parameters:\n",
    "      maps: np.array of model probabilities (N x num_classes)\n",
    "      label_mapping: dict mapping labels to indices\n",
    "      smiles (optional): np.array of SMILES strings for each sample\n",
    "      lab (optional): np.array of ground-truth labels for each sample\n",
    "    \"\"\"\n",
    "    for i in range(len(maps)):\n",
    "        print(\"Sample #\", i)\n",
    "        if smiles is not None:\n",
    "            print(\"  SMILES:   \", smiles[i])\n",
    "            \n",
    "        # Convert each probability to a percentage string with 5 decimal places\n",
    "        percentage_probs = [f\"{p * 100:.5f}%\" for p in maps[i]]\n",
    "        \n",
    "        # Create a dictionary mapping each label (with index) to its corresponding probability percentage\n",
    "        labeled_percentage_probs = {\n",
    "            f\"{label}({idx})\": percentage_probs[idx] \n",
    "            for label, idx in label_mapping.items()\n",
    "        }\n",
    "        \n",
    "        # Identify the index of the highest probability for this sample\n",
    "        max_index = maps[i].argmax()\n",
    "        highest_prob = maps[i][max_index]\n",
    "        \n",
    "        # Get the corresponding label for the highest probability, with a default if not found\n",
    "        highest_label = next((label for label, idx in label_mapping.items() if idx == max_index), f\"Index {max_index}\")\n",
    "        \n",
    "        # Create a dictionary for the highest probability using the \"label(index)\" format\n",
    "        labeled_percentage_highest_probs = {\n",
    "            f\"{highest_label}({max_index})\": f\"{highest_prob * 100:.1f}%\"\n",
    "        }\n",
    "        \n",
    "        print(\"  Labeled ProbDist %:\", labeled_percentage_probs)\n",
    "        print(\"  Highest Probability:\", labeled_percentage_highest_probs)\n",
    "        print()  # Empty line for readability\n",
    "\n",
    "# --- Example usage with the fully trained model branch ---\n",
    "use_fully_trained_model = False\n",
    "\n",
    "if use_fully_trained_model:\n",
    "    checkpoint = torch.load(os.path.join(\"PharmaNet/Best_Config/model1\", 'test_predictions.pth'))\n",
    "    \n",
    "    \n",
    "    # here we get the original label mapping\n",
    "    df = pd.read_csv('data/datasets/DUDE/Smiles_1.csv')\n",
    "    unique_targets = df.drop_duplicates(subset='Target')\n",
    "    label_mapping = dict(zip(unique_targets['Target'], unique_targets['Label']))\n",
    "    \n",
    "    # Extract the 'prediction' entry which contains [smiles, maps, lab]\n",
    "    predictions_map = checkpoint['prediction']\n",
    "    smiles = predictions_map[0]  # np.array of SMILES strings\n",
    "    maps   = predictions_map[1]  # np.array of model probabilities (N x num_classes)\n",
    "    lab    = predictions_map[2]  # np.array of ground-truth class labels\n",
    "\n",
    "    print(\"SMILES array shape:\", smiles.shape)\n",
    "    print(\"Probability array shape:\", maps.shape)\n",
    "    print(\"Labels array shape:\", lab.shape)\n",
    "    \n",
    "    # Use the reusable function to display predictions for each sample\n",
    "    display_predictions(maps, label_mapping, smiles=smiles, lab=lab)\n",
    "    # plot_predictions(maps, label_mapping, path=\"PharmaNet/Best_Config/model1\")\n",
    "    \n",
    "else:\n",
    "    # Use an alternative checkpoint path\n",
    "    label_mapping = {'fabp4': 0, 'kith': 1, 'cxcr4': 2, 'comt': 3, 'hmdh': 4, 'akt1': 5, 'ada17': 6}\n",
    "    checkpoint = torch.load(os.path.join(saving_path_models, 'test_predictions.pth'))\n",
    "    \n",
    "    # Extract the 'prediction' entry (here, we assume it returns a dict with key 'outputs')\n",
    "    predictions_map = checkpoint['prediction']\n",
    "    maps = predictions_map['outputs']  # np.array of model probabilities (N x num_classes)\n",
    "    \n",
    "    print(\"Probability array shape:\", maps.shape)\n",
    "    \n",
    "    # Use the reusable function without SMILES and ground-truth labels\n",
    "    display_predictions(maps, label_mapping)\n",
    "    plot_predictions(maps, label_mapping, path=saving_path_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PharmaNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
