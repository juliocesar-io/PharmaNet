{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **IA Biomédica:** Laboratorio PharmaNet: Funciones Avanzadas\n",
    "\n",
    "\n",
    "> ##### Descubrimiento de Farmacos\n",
    "\n",
    "\n",
    "> **Instructores**\n",
    "\n",
    "*  Julio Castellanos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Laboratorio: Uso Avanzado para el Descubrimiento de Fármacos\n",
    "\n",
    "En este laboratorio profundizaremos en la interpretación de los resultados de inferencia y modificaremos el modelo original para entrenarlo con nuestros propios targets.\n",
    "\n",
    "Al final de esta sesión, se espera haber logrado lo siguiente:\n",
    "\n",
    "- Entrenar un modelo exclusivamente con datos de targets seleccionados (i.e: proteínas como Thymidine Kinase (KITH) y Fatty Acid-Binding Protein (FABP4)).\n",
    "- Hacer plots de la la función de perdida y NAP\n",
    "- Realizar el preprocesamiento de los resultados de inferencia para mejorar su interpretación.\n",
    "- Validar los datos en UniProt y hacer interpretación estructural con AlphaFold y Boltz-1 en Fastfold.\n",
    "\n",
    "\n",
    "### Referencias:\n",
    "\n",
    "- PharmaNet: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0241728\n",
    "- KITH: https://www.uniprot.org/uniprotkb/P04183/entry#sequences\n",
    "- FABP4: https://www.uniprot.org/uniprotkb/P04183/entry#sequences\n",
    "- Alphafold: https://www.nature.com/articles/s41586-024-07487-w\n",
    "- Boltz-1: https://www.biorxiv.org/content/10.1101/2024.11.19.624167v1\n",
    "- Fastfold: https://www.fastfold.ai\n",
    "\n",
    "### Modelo\n",
    "\n",
    "<p align=\"center\"><img src=\"Overview.png\" /></p>\n",
    "\n",
    "\n",
    "### Top Targets \n",
    "\n",
    "<p align=\"center\"><img src=\"data/lab/top_targets.png\" /></p>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliocesar/miniforge3/envs/PharmaNet/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.9.0.post2\n"
     ]
    }
   ],
   "source": [
    "# ## 1. Importaciones Principales\n",
    "\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Modulos propios (asegúrate de tener estas rutas en tu proyecto)\n",
    "from data.data_loader import SMILESDataset\n",
    "from utils.metrics import pltauc, norm_ap_optimized\n",
    "from models.Model import Model\n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración de Parámetros y Semilla\n",
    "\n",
    "En esta sección, definimos los parámetros necesarios para la ejecución:\n",
    "- GPU o CPU\n",
    "- Fijación de semilla aleatoria para reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo seleccionado: cpu\n"
     ]
    }
   ],
   "source": [
    "# 2. Configuración de Parámetros y Semilla\n",
    "\n",
    "seed = 6766\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Configurar GPU (si está disponible)\n",
    "ngpu = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(ngpu)\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "print(\"Dispositivo seleccionado:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lectura de Datos\n",
    "\n",
    "Tomamos los datos y filtramos las filas para mantener solo los registros con Target “kith” o “fabp4”, convierte esos valores a etiquetas numéricas (1 para “kith” y 0 para “fabp4”), ordena los datos por la etiqueta y finalmente guarda los resultados filtrados y ordenados en archivos CSV separados para entrenamiento y prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for train: 792\n",
      "Data for test : 274\n"
     ]
    }
   ],
   "source": [
    "# 3. Lectura de Datos\n",
    "train_path = \"data/datasets/DUDE/\"\n",
    "A = pd.read_csv(train_path + 'Smiles_1.csv')\n",
    "B = pd.read_csv(train_path + 'Smiles_2.csv')\n",
    "C = pd.read_csv(train_path + 'Smiles_3.csv')\n",
    "D = pd.read_csv(train_path + 'Smiles_4.csv')\n",
    "\n",
    "data_train = pd.concat([A, B, C], ignore_index=True)\n",
    "data_test = D\n",
    "\n",
    "# Filtrado de los datos para conservar solo las filas con Targets deseados\n",
    "targets_to_keep = ['kith', 'fabp4', 'cxcr4', 'comt', 'hmdh', 'akt1', 'ada17']\n",
    "label_mapping = {'fabp4': 0, 'kith': 1, 'cxcr4': 2, 'comt': 3, 'hmdh': 4, 'akt1': 5, 'ada17': 6}\n",
    "\n",
    "# Para data_train: filtrar, seleccionar las columnas deseadas, actualizar la columna Label y ordenar\n",
    "filtered_train = data_train[data_train['Target'].isin(targets_to_keep)][['Smiles', 'Target', 'Label']].copy()\n",
    "filtered_train['Label'] = filtered_train['Target'].map(label_mapping)\n",
    "filtered_train = filtered_train.sort_values(by='Label')\n",
    "filtered_train.to_csv(train_path + \"LAB_TRAIN/train_data.csv\", index=False)\n",
    "\n",
    "# Para data_test: filtrar, seleccionar las columnas deseadas, actualizar la columna Label y ordenar\n",
    "filtered_test = data_test[data_test['Target'].isin(targets_to_keep)][['Smiles', 'Target', 'Label']].copy()\n",
    "filtered_test['Label'] = filtered_test['Target'].map(label_mapping)\n",
    "filtered_test = filtered_test.sort_values(by='Label')\n",
    "filtered_test.to_csv(train_path + \"LAB_TRAIN/test_data.csv\", index=False)\n",
    "\n",
    "\n",
    "# Filtrar y guardar datos para entrenamiento\n",
    "filtered_train = data_train[data_train['Target'].isin(targets_to_keep)][['Smiles', 'Target', 'Label']].copy()\n",
    "filtered_train['Label'] = filtered_train['Target'].map(label_mapping)\n",
    "filtered_train = filtered_train.sort_values(by='Label')\n",
    "filtered_train = filtered_train.reset_index(drop=True)  # Reset index\n",
    "filtered_train.to_csv(os.path.join(train_path, \"LAB_TRAIN/train_data.csv\"), index=False)\n",
    "\n",
    "# Filtrar y guardar datos para test\n",
    "filtered_test = data_test[data_test['Target'].isin(targets_to_keep)][['Smiles', 'Target', 'Label']].copy()\n",
    "filtered_test['Label'] = filtered_test['Target'].map(label_mapping)\n",
    "filtered_test = filtered_test.sort_values(by='Label')\n",
    "filtered_test = filtered_test.reset_index(drop=True)  # Reset index\n",
    "filtered_test.to_csv(os.path.join(train_path, \"LAB_TRAIN/test_data.csv\"), index=False)\n",
    "\n",
    "# Luego, al asignar los datos para el dataset:\n",
    "data_train = filtered_train\n",
    "data_test = filtered_test\n",
    "\n",
    "print('Data for train:', len(data_train['Smiles']))\n",
    "print('Data for test :', len(data_test['Smiles']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparación del Vocabulario (charset) y Definición de Longitud Máxima\n",
    "Extraemos un conjunto de caracteres únicos usados en los SMILES y calculamos la longitud máxima de la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caracteres únicos: {'B', '=', '7', '8', '1', '@', 'S', 'O', 'o', '-', '4', 'I', '(', 'l', 'P', '/', 'r', '\\\\', 's', '2', 'N', '[', 'C', '6', ')', '5', 'n', 'c', 'H', ']', 'F', '#', '3'}\n",
      "Total de caracteres: 33\n",
      "Max. longitud de SMILE: 112\n"
     ]
    }
   ],
   "source": [
    "# 4. Vocabulario y Embedding\n",
    "charset = set(\"\".join(list(data_train.Smiles)) + \"\".join(list(data_test.Smiles)))\n",
    "vocab_size = len(charset)\n",
    "char_to_int = dict((c, i) for i, c in enumerate(charset))\n",
    "\n",
    "embed_tr = max([len(smile) for smile in data_train.Smiles])\n",
    "embed_te = max([len(smile) for smile in data_test.Smiles])\n",
    "embed = max(embed_tr, embed_te)\n",
    "\n",
    "print('Caracteres únicos:', str(charset))\n",
    "print('Total de caracteres:', vocab_size)\n",
    "print('Max. longitud de SMILE:', embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construcción del Modelo (PharmaNet)\n",
    "\n",
    "Se instancia la clase `Model(...)` que define la arquitectura Conv + RNN.  \n",
    "Luego se imprime el modelo para revisar su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Model(\n",
      "  (module_mol): ModuleList(\n",
      "    (0): Conv2d(33, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(3, 2))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): GRU(128, 256, num_layers=10, batch_first=True, bidirectional=True)\n",
      "    (5): Upsample(size=64, mode=nearest)\n",
      "  )\n",
      "  (fc1): Linear(in_features=512, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Construcción del Modelo\n",
    "num_classes = max(data_train.Label) + 1\n",
    "print(num_classes)\n",
    "hidden_size = 256\n",
    "kernel_size = 5\n",
    "\n",
    "bidireccional = True\n",
    "num_layers = 10\n",
    "\n",
    "net = Model(vocab_size,\n",
    "            num_classes,\n",
    "            hidden_size,\n",
    "            bidireccional,\n",
    "            num_layers,\n",
    "            kernel_size\n",
    "            ).to(device)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Definición del Optimizador y la Función de Pérdida\n",
    "\n",
    "Usamos Adam como optimizador y BCELoss (aplicando `sigmoid` en cada clase).  \n",
    "El scheduler reduce el learning rate si no hay mejoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Optimizador y Pérdida\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=7)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creación de los DataLoaders (Train/Test)\n",
    "\n",
    "Se definen:\n",
    "- El dataset con la clase `SMILESDataset`.\n",
    "- DataLoader con sampler balanceado o shuffle, según los argumentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader y test loader creados.\n"
     ]
    }
   ],
   "source": [
    "# 7. DataLoaders\n",
    "\n",
    "neighbours = 0\n",
    "padding = False\n",
    "\n",
    "batch_size = 128\n",
    "workers = 5\n",
    "\n",
    "train_datasets = SMILESDataset(data_train, vocab_size, char_to_int, embed, neighbours, padding)\n",
    "test_datasets  = SMILESDataset(data_test, vocab_size, char_to_int, embed, neighbours, padding)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_datasets, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=workers)\n",
    "test_loader  = DataLoader(test_datasets,  batch_size=batch_size, drop_last=True, shuffle=True, num_workers=workers)\n",
    "\n",
    "print(\"Train loader y test loader creados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Funciones de Entrenamiento y Evaluación\n",
    "\n",
    "- **train(epoch)**: Modo `train()`, calcula pérdida y backprop.\n",
    "- **evaluate(epoch)**: Modo `eval()`, sin backprop, y se calculan métricas finales (NAP, AUC, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    # Coloca el modelo en modo entrenamiento (permite dropout, actualiza gradientes)\n",
    "    net.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    lab = np.array([])                       # Para almacenar etiquetas reales\n",
    "    maps = np.empty((0, num_classes))        # Para almacenar predicciones (probabilidades)\n",
    "\n",
    "    # Iteramos sobre cada batch (inputs, _, labels) proveniente de train_loader\n",
    "    for inputs, _, labels in tqdm(train_loader):\n",
    "        # Convertimos a tensores en float32 y los movemos al dispositivo (CPU/GPU)\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Reiniciamos gradientes antes de cada batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Activamos el cálculo de gradientes\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Forward pass: obtenemos outputs de la red\n",
    "            outputs = net(inputs)\n",
    "            loss = 0\n",
    "\n",
    "            # Para cada clase, calculamos el aporte a la pérdida usando BCELoss\n",
    "            for i in range(num_classes):\n",
    "                labe = labels.clone()\n",
    "                labe[labels == i] = 1\n",
    "                labe[labels != i] = 0\n",
    "                loss += criterion(F.sigmoid(outputs[:, i].float()), labe.float().to(device))\n",
    "\n",
    "            # Convertimos outputs a numpy para calcular métricas (softmax de outputs)\n",
    "            outputs_array = F.softmax(outputs.clone().detach().cpu(), dim=1).numpy()\n",
    "            maps = np.append(maps, outputs_array, axis=0)\n",
    "\n",
    "            # Guardamos las etiquetas en un array para métricas\n",
    "            labels_array = labels.clone().cpu().detach().numpy()\n",
    "            lab = np.append(lab, labels_array)\n",
    "\n",
    "            # Obtenemos la predicción (clase con mayor probabilidad)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Backprop: calculamos gradientes y actualizamos pesos\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Acumulamos pérdida total del batch\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Calculamos aciertos para accuracy\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Pérdida media en la época\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    # Precisión en la época\n",
    "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "    # Cálculo de métricas: NAP y AUC (incluye macro y micro)\n",
    "    epoch_nap, epoch_f = norm_ap_optimized(maps, lab, num_classes)\n",
    "    epoch_auc = pltauc(maps, lab, num_classes)\n",
    "\n",
    "    # Imprimimos métricas clave\n",
    "    print('Phase Train, Loss: {:.4f} Acc: {:.4f} NAP: {:.4f} F-measure: {:.4f} AUC: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc, epoch_nap[-1], epoch_f[-1], epoch_auc[\"micro\"]))\n",
    "\n",
    "    # Retornamos valores relevantes para graficar o almacenar\n",
    "    return epoch_loss, epoch_acc, epoch_nap[-1], epoch_f[-1], epoch_auc[\"micro\"]\n",
    "\n",
    "\n",
    "def evaluate(epoch):\n",
    "    net.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = torch.tensor(0.0, device=device)  # Changed initialization\n",
    "\n",
    "    lab = np.array([])\n",
    "    maps = np.empty((0, num_classes))\n",
    "\n",
    "    # Iterate over validation data\n",
    "    for inputs, _, labels in test_loader:\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = net(inputs)\n",
    "            loss = 0\n",
    "            for i in range(num_classes):\n",
    "                labe = labels.clone()\n",
    "                labe[labels == i] = 1\n",
    "                labe[labels != i] = 0\n",
    "                loss += criterion(F.sigmoid(outputs[:, i].float()), labe.float().to(device))\n",
    "\n",
    "            outputs_array = F.softmax(outputs.clone().detach().cpu(), dim=1).numpy()\n",
    "            maps = np.append(maps, outputs_array, axis=0)\n",
    "            labels_array = labels.clone().cpu().detach().numpy()\n",
    "            lab = np.append(lab, labels_array)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(test_loader.dataset)\n",
    "    \n",
    "    # Update scheduler using the aggregated loss\n",
    "    scheduler.step(epoch_loss)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(param_group['lr'])\n",
    "        \n",
    "    epoch_acc = running_corrects.double() / len(test_loader.dataset)\n",
    "    epoch_nap, epoch_f = norm_ap_optimized(maps, lab, num_classes)                \n",
    "    epoch_auc = pltauc(maps, lab, num_classes)\n",
    "    \n",
    "    predictions = [maps, lab]\n",
    "    \n",
    "    print('Phase Validation, Loss: {:.4f} Acc: {:.4f} NAP: {:.4f} F-measure: {:.4f} auc:  {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc, epoch_nap[-1], epoch_f[-1], epoch_auc[\"micro\"]))\n",
    "    \n",
    "    return epoch_loss, epoch_acc, epoch_nap[-1], epoch_f[-1], epoch_auc[\"micro\"], predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plots de Metricas \n",
    "\n",
    "Graficamos la metricas para hacer seguimiento al progreso en tiempo de ejecucion del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_loss(d_losses, g_losses, save_dir, num_epoch, save=True, show=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0,num_epoch)\n",
    "    ax.set_ylim(0, max(np.max(g_losses), np.max(d_losses))*1.1)\n",
    "    plt.xlabel('Epoch {0}'.format(num_epoch))\n",
    "    plt.ylabel('Loss values')\n",
    "    plt.plot(d_losses, label='Train')\n",
    "    plt.plot(g_losses, label='Test')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_fn = save_dir + 'losses_{:d}'.format(num_epoch) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "def plot_nap(d_losses, g_losses, f_losses, td_losses, tg_losses, tf_losses, save_dir, num_epoch, save=True, show=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0,num_epoch)\n",
    "    ax.set_ylim(0, max(np.max(g_losses), np.max(d_losses), np.max(f_losses))*1.1)\n",
    "    plt.xlabel('Epoch {0}'.format(num_epoch))\n",
    "    plt.ylabel('metric values')\n",
    "    plt.plot(d_losses, label='Test nap')\n",
    "    plt.plot(g_losses, label='Test AUC')\n",
    "    plt.plot(f_losses, label='Test accuracy')\n",
    "    plt.plot(td_losses, label='Train nap')\n",
    "    plt.plot(tg_losses, label='Train AUC')\n",
    "    plt.plot(tf_losses, label='Train accuracy')\n",
    "    plt.legend()\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_fn = save_dir + 'nap{:d}'.format(num_epoch) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Bucle Principal de Entrenamiento\n",
    "\n",
    "Controla el número de épocas, guarda el mejor modelo según la métrica (NAP, por ejemplo) y produce gráficas de evolución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0\n",
    "num_epochs = 2\n",
    "\n",
    "results_dir = './PharmaNet'\n",
    "results_path = 'Lab_Advanced'\n",
    "\n",
    "# Historial para gráficas\n",
    "train_losses_history, val_losses_history = [], []\n",
    "train_acc_history, val_acc_history = [], []\n",
    "train_nap_history, val_nap_history = [], []\n",
    "train_f_history, val_f_history = [], []\n",
    "train_auc_history, val_auc_history = [], []\n",
    "\n",
    "best_model_wts = copy.deepcopy(net.state_dict())\n",
    "saving_path_models = os.path.join(results_dir, results_path, 'model_1'+'/')\n",
    "if not os.path.exists(saving_path_models):\n",
    "    os.makedirs(saving_path_models, 0o777)\n",
    "\n",
    "def main():\n",
    "    since = time.time()\n",
    "    best_NAP = 0\n",
    "    best_prediction = None\n",
    "\n",
    "    for epoch in range(initial_epoch, num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Entrenar\n",
    "        train_loss, train_acc, train_nap, train_f, train_auc = train(epoch)\n",
    "        train_losses_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "        train_auc_history.append(train_auc)\n",
    "        train_nap_history.append(train_nap)\n",
    "        train_f_history.append(train_f)\n",
    "\n",
    "        # Validar\n",
    "        val_loss, val_acc, val_nap, val_f, val_auc, prediction_nap = evaluate(epoch)\n",
    "        val_losses_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "        val_auc_history.append(val_auc)\n",
    "        val_nap_history.append(val_nap)\n",
    "        val_f_history.append(val_f)\n",
    "\n",
    "        # Guardar el mejor modelo si la métrica (NAP) mejora\n",
    "        if val_nap > best_NAP:\n",
    "            best_NAP = val_nap\n",
    "            best_prediction = prediction_nap\n",
    "            best_model_wts = copy.deepcopy(net.state_dict())\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_losses_history\n",
    "            }, os.path.join(saving_path_models, 'Checkpoint.pth'))\n",
    "            \n",
    "        # Graficar loss y nap\n",
    "        \n",
    "        plot_loss(train_losses_history,\n",
    "                val_losses_history, \n",
    "                saving_path_models,\n",
    "                num_epochs,\n",
    "                show=True)\n",
    "        plot_nap(val_nap_history, val_auc_history, \n",
    "                torch.tensor(val_acc_history).cpu().tolist(),\n",
    "                train_nap_history, train_auc_history, \n",
    "                torch.tensor(train_acc_history).cpu().tolist(),\n",
    "                saving_path_models,\n",
    "                num_epochs,\n",
    "                show=True)   \n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val NAP: {:4f}'.format(best_NAP))\n",
    "    print('Best val ACC: {:4f}'.format(max(val_acc_history)))\n",
    "    print('Best val AUC: {:4f}'.format(max(val_auc_history)))\n",
    "    print('Best val Fscore: {:4f}'.format(max(val_f_history)))\n",
    "\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    torch.save({\n",
    "        'model': net.state_dict(),\n",
    "        'optimize': optimizer.state_dict(),\n",
    "        'prediction': best_prediction,\n",
    "        'charset': char_to_int,\n",
    "        'embed': embed\n",
    "    }, os.path.join(saving_path_models, 'model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ejecucion de Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:02<00:00, 30.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase Train, Loss: 2.9944 Acc: 0.3333 NAP: 0.2771 F-measure: 0.2747 AUC: 0.4942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n",
      "Phase Validation, Loss: 2.2323 Acc: 0.4161 NAP: 0.3827 F-measure: 0.4133 auc:  0.5358\n",
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:15<00:00, 32.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase Train, Loss: 2.2716 Acc: 0.4381 NAP: 0.3012 F-measure: 0.2979 AUC: 0.5140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n",
      "Phase Validation, Loss: 2.2336 Acc: 0.4161 NAP: 0.2783 F-measure: 0.3258 auc:  0.5025\n",
      "Training complete in 7m 18s\n",
      "Best val NAP: 0.382696\n",
      "Best val ACC: 0.416058\n",
      "Best val AUC: 0.535819\n",
      "Best val Fscore: 0.413256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_train = True\n",
    "\n",
    "if run_train:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inferencia usando Torch Save\n",
    "\n",
    "**Preparación de predicciones**\n",
    "Tras la inferencia, se acumulan las salidas softmax (all_outputs), las etiquetas reales (all_labels) y se calculan las predicciones con np.argmax para determinar la etiqueta con mayor probabilidad.\n",
    "\n",
    "**Guardado con torch.save**\n",
    "Se crea un diccionario llamado predictions que contiene las salidas, las etiquetas reales y las etiquetas predichas. Luego, este diccionario se guarda en un archivo llamado test_predictions.pth en la ruta saving_path_models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde: ./PharmaNet/Lab_Advanced/model_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia: 100%|██████████| 3/3 [00:35<00:00, 11.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RESULTADOS DE INFERENCIA]\n",
      "  NAP: 0.2722\n",
      "  AP (micro): 0.1582\n",
      "  AUC (micro): 0.5937\n",
      "  Accuracy: 0.4453\n",
      "\n",
      "Final predictions saved to: ./PharmaNet/Lab_Advanced/model_1/test_predictions.pth\n",
      "{'NAP': 0.2721622513264338, 'AP_micro': 0.15823164987230323, 'AUC_micro': 0.5937184595146274, 'Accuracy': 0.44525547445255476}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.data_loader import SMILESDataset\n",
    "from utils.metrics import pltmap, pltauc, norm_ap\n",
    "from models.Model import Model\n",
    "\n",
    "def inference_example(\n",
    "    data_test,           # DataFrame con SMILES y etiquetas\n",
    "    saving_path_models,  # Ruta donde está el modelo entrenado (model.pth)\n",
    "    device,              # \"cuda\" o \"cpu\"\n",
    "    num_classes,         # Cantidad de clases/targets esperadas en el modelo\n",
    "    hidden_size,         # Tamaño hidden para la red\n",
    "    bidireccional,       # Boolean, si la RNN es bidireccional\n",
    "    num_layers,          # Número de capas en la RNN\n",
    "    kernel_size,         # Tamaño kernel para convolución\n",
    "    neighbours,          # Parámetro extra para SMILESDataset\n",
    "    padding,             # Padding usado en SMILESDataset\n",
    "    batch_size,          # Tamaño de batch para DataLoader\n",
    "    workers              # Num. de workers para DataLoader\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejemplo simplificado de inferencia con PharmaNet.\n",
    "    - data_test: DataFrame con columnas ['Smiles','Label']\n",
    "    - Se asume que 'model.pth' contiene ['model','charset','embed'].\n",
    "    \"\"\"\n",
    "    print(\"Cargando modelo desde:\", saving_path_models)\n",
    "    model_path = os.path.join(saving_path_models, 'model.pth')\n",
    "    trained_model = torch.load(model_path, map_location=device)\n",
    "\n",
    "    # Reconstruir diccionario de caracteres y embed\n",
    "    char_to_int_inf = trained_model['charset']\n",
    "    embed_inf       = trained_model['embed']\n",
    "    vocab_size_inf  = len(char_to_int_inf)\n",
    "\n",
    "    # Reconstruir el modelo con los mismos hiperparámetros\n",
    "    model_infer = Model(\n",
    "        vocab_size_inf,\n",
    "        num_classes,\n",
    "        hidden_size,\n",
    "        bidireccional,\n",
    "        num_layers,\n",
    "        kernel_size\n",
    "    ).to(device)\n",
    "\n",
    "    model_infer.load_state_dict(trained_model['model'])\n",
    "    model_infer.eval()\n",
    "\n",
    "    # Crear dataset de prueba\n",
    "    test_dataset_inf = SMILESDataset(\n",
    "        data_test,\n",
    "        vocab_size_inf,\n",
    "        char_to_int_inf,\n",
    "        embed_inf,\n",
    "        neighbours,\n",
    "        padding\n",
    "    )\n",
    "\n",
    "    test_loader_inf = DataLoader(\n",
    "        test_dataset_inf,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=workers\n",
    "    )\n",
    "\n",
    "    all_outputs = np.empty((0, num_classes))\n",
    "    all_labels  = np.array([])\n",
    "\n",
    "    # Bucle de inferencia\n",
    "    for inputs, _, labels in tqdm(test_loader_inf, desc=\"Inferencia\"):\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_infer(inputs)    # (batch_size, num_classes)\n",
    "            out_soft = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "            all_outputs = np.append(all_outputs, out_soft, axis=0)\n",
    "\n",
    "            # Guardamos las etiquetas reales para métricas\n",
    "            labels_np = labels.cpu().numpy()\n",
    "            all_labels = np.append(all_labels, labels_np)\n",
    "\n",
    "    # ===================\n",
    "    # Cálculo de métricas\n",
    "    # ===================\n",
    "    unique_classes = np.unique(all_labels)\n",
    "    if len(unique_classes) < num_classes:\n",
    "        print(f\"Warning: Se esperaban {num_classes} clases, pero solo se encontraron {len(unique_classes)} en el test.\")\n",
    "    \n",
    "    # Intentamos calcular cada métrica y, si falla, asignamos un valor por defecto.\n",
    "    try:\n",
    "        epoch_nap, epoch_f = norm_ap(all_outputs, all_labels, num_classes=num_classes)\n",
    "    except Exception as e:\n",
    "        print(\"Error al calcular NAP:\", e)\n",
    "        epoch_nap, epoch_f = [0.0], [0.0]\n",
    "    \n",
    "    try:\n",
    "        epoch_ap, _ = pltmap(all_outputs, all_labels, num_classes=num_classes)\n",
    "    except Exception as e:\n",
    "        print(\"Error al calcular MAP:\", e)\n",
    "        epoch_ap = {\"micro\": 0.0}\n",
    "    \n",
    "    try:\n",
    "        # Si hay menos de dos clases presentes, evitamos el cálculo de AUC\n",
    "        if len(unique_classes) < 2:\n",
    "            print(\"Solo hay una clase en los datos. No se puede calcular AUC.\")\n",
    "            epoch_auc = {\"micro\": 0.0, \"macro\": 0.0}\n",
    "        else:\n",
    "            epoch_auc = pltauc(all_outputs, all_labels, num_classes=num_classes)\n",
    "    except Exception as e:\n",
    "        print(\"Error al calcular AUC:\", e)\n",
    "        epoch_auc = {\"micro\": 0.0, \"macro\": 0.0}\n",
    "\n",
    "    # Accuracy\n",
    "    preds = np.argmax(all_outputs, axis=1)\n",
    "    running_corrects = np.sum(preds == all_labels)\n",
    "    epoch_acc = running_corrects / len(all_labels) if len(all_labels) > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n[RESULTADOS DE INFERENCIA]\")\n",
    "    print(f\"  NAP: {epoch_nap[-1]:.4f}\")\n",
    "    print(f\"  AP (micro): {epoch_ap['micro']:.4f}\")\n",
    "    print(f\"  AUC (micro): {epoch_auc['micro']:.4f}\")\n",
    "    print(f\"  Accuracy: {epoch_acc:.4f}\\n\")\n",
    "\n",
    "    # Preparar un diccionario con las predicciones finales\n",
    "    predictions = {\n",
    "        'outputs': all_outputs,           # Salidas softmax para cada muestra\n",
    "        'labels': all_labels,             # Etiquetas reales\n",
    "        'predicted_labels': preds         # Etiquetas predichas (argmax)\n",
    "    }\n",
    "\n",
    "    # Guardar las predicciones finales usando torch.save\n",
    "    save_path = os.path.join(saving_path_models, 'test_predictions.pth')\n",
    "    torch.save({\n",
    "        'prediction': predictions\n",
    "    }, save_path)\n",
    "    print(\"Final predictions saved to:\", save_path)\n",
    "\n",
    "    return {\n",
    "        \"NAP\": epoch_nap[-1],\n",
    "        \"AP_micro\": epoch_ap[\"micro\"],\n",
    "        \"AUC_micro\": epoch_auc[\"micro\"],\n",
    "        \"Accuracy\": epoch_acc\n",
    "    }\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# Se asume que ya tienes definidos data_test, saving_path_models, device, num_classes,\n",
    "# hidden_size, bidireccional, num_layers, kernel_size, neighbours, padding, batch_size y workers.\n",
    "resultados = inference_example(\n",
    "     data_test=data_test,\n",
    "     saving_path_models=saving_path_models,\n",
    "     device=\"cpu\",\n",
    "     num_classes=num_classes,\n",
    "     hidden_size=hidden_size,\n",
    "     bidireccional=bidireccional,\n",
    "     num_layers=num_layers,\n",
    "     kernel_size=kernel_size,\n",
    "     neighbours=neighbours,\n",
    "     padding=padding,\n",
    "     batch_size=batch_size,\n",
    "     workers=workers\n",
    ")\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia a targets especificos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Target</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nc1ncnc2n(cnc12)[C@@H]1O[C@H](COP([O-])(=O)OP(...</td>\n",
       "      <td>kith</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[H][C@]12CC[C@]3(C)[C@@H](O)CC[C@@]3([H])[C@]1...</td>\n",
       "      <td>comt</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Smiles Target  Label\n",
       "0  Nc1ncnc2n(cnc12)[C@@H]1O[C@H](COP([O-])(=O)OP(...   kith      1\n",
       "1  [H][C@]12CC[C@]3(C)[C@@H](O)CC[C@@]3([H])[C@]1...   comt      3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_validation = pd.read_csv('data/datasets/DUDE/LAB_TRAIN/inference_validation.csv')\n",
    "data_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Proteina 1**\n",
    "Target: Thymidine Kinase (KITH)\n",
    "\n",
    "<p align=\"center\"><img style=\"width: 350px\" src=\"https://raw.githubusercontent.com/juliocesar-io/PharmaNet/refs/heads/main/data/lab/KITH.png\" /></p>\n",
    "\n",
    "\n",
    "Fuente: Resultado prediccion en https://cloud.fastfold.ai con Boltz-1\n",
    "\n",
    "Sequencia: \n",
    "\n",
    "```\n",
    "MSCINLPTVLPGSPSKTRGQIQVILGPMFSGKSTELMRRVRRFQIAQYKCLVIKYAKDTRYSSSFCTHDRNTMEALPACLLRDVAQEALGVAVIGIDEGQFFPDIVEFCEAMANAGKTVIVAALDGTFQRKPFGAILNLVPLAESVVKLTAVCMECFREAAYTKRLGTEKEVEVIGGADKYHSVCRLCYFKKASGQPAGPDNKENCPVPGKPGEAVAARKLFAPQQILQCSPAN\n",
    "```\n",
    "\n",
    "\n",
    "**Molecula (SMILES) 1**\n",
    "\n",
    "ATP\n",
    "\n",
    "<p align=\"center\"><img style=\"width: 150px\" src=\"https://raw.githubusercontent.com/juliocesar-io/PharmaNet/refs/heads/main/data/lab/atp.png\" /></p>\n",
    "\n",
    "Secuencia:\n",
    "```\n",
    "Nc1ncnc2n(cnc12)[C@@H]1O[C@H](COP([O-])(=O)OP([O-])(=O)OP([O-])([O-])=O)[C@@H](O)[C@H]1O\n",
    "```\n",
    "\n",
    "Ref: https://www.uniprot.org/uniprotkb/P04183/entry#sequences\n",
    "\n",
    "\n",
    "**Proteina 2**\n",
    "Target: Catechol O-methyltransferase (COMT)\n",
    "\n",
    "<p align=\"center\"><img style=\"width: 350px\" src=\"https://raw.githubusercontent.com/juliocesar-io/PharmaNet/refs/heads/main/data/lab/COMT.png\" /></p>\n",
    "\n",
    "\n",
    "Fuente: Resultado prediccion en https://cloud.fastfold.ai con Boltz-1\n",
    "\n",
    "Sequencia: \n",
    "\n",
    "```\n",
    "MGDTKEQRILNHVLQHAEPGNAQSVLEAIDTYCEQKEWAMNVGDKKGKIVGAVIQEHQPFVLLELGAYCGYSAVRMARLLSPGARLITIEINPDCAAITQRMVDFAGVKDKVTLVVGASQDIIPQLKKKYDVDTLDMVFLDHWKDRYLPDTLLLEECGLLRRGTVLLADNVICPGAPDFLAHVRGSSCFECTHYQSFLEYREVVDGLEKAIYKGPGSEAGP\n",
    "```\n",
    "\n",
    "**Molecula (SMILES) 2**\n",
    "\n",
    "S-adenosyl-L-methionine zwitterion\n",
    "\n",
    "<p align=\"center\"><img style=\"width: 150px\" src=\"https://raw.githubusercontent.com/juliocesar-io/PharmaNet/refs/heads/main/data/lab/S-adenosyl-L-methionine.png\" /></p>\n",
    "\n",
    "Secuencia:\n",
    "```\n",
    "C[S+](CC[C@H]([NH3+])C([O-])=O)C[C@H]1O[C@H]([C@H](O)[C@@H]1O)n1cnc2c(N)ncnc12\n",
    "```\n",
    "\n",
    "Ref: https://www.uniprot.org/uniprotkb/P04183/entry#sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde: ./PharmaNet/Lab_Advanced/model_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia: 100%|██████████| 1/1 [00:06<00:00,  6.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Se esperaban 7 clases, pero solo se encontraron 2 en el test.\n",
      "Error al calcular AUC: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "[RESULTADOS DE INFERENCIA]\n",
      "  NAP: nan\n",
      "  AP (micro): 0.2143\n",
      "  AUC (micro): 0.0000\n",
      "  Accuracy: 0.0000\n",
      "\n",
      "Final predictions saved to: ./PharmaNet/Lab_Advanced/model_1/test_predictions.pth\n",
      "{'NAP': nan, 'AP_micro': 0.21428571428571427, 'AUC_micro': 0.0, 'Accuracy': 0.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_validation = pd.read_csv('data/datasets/DUDE/LAB_TRAIN/inference_validation.csv')\n",
    "\n",
    "resultados = inference_example(\n",
    "     data_test=data_validation,\n",
    "     saving_path_models=saving_path_models,\n",
    "     device=\"cpu\",\n",
    "     num_classes=num_classes,\n",
    "     hidden_size=hidden_size,\n",
    "     bidireccional=bidireccional,\n",
    "     num_layers=num_layers,\n",
    "     kernel_size=kernel_size,\n",
    "     neighbours=neighbours,\n",
    "     padding=padding,\n",
    "     batch_size=batch_size,\n",
    "     workers=workers\n",
    ")\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Interpretacion Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability array shape: (2, 7)\n",
      "Sample # 0\n",
      "  Labeled ProbDist %: {'fabp4(0)': '4.23546%', 'kith(1)': '4.77052%', 'cxcr4(2)': '3.73392%', 'comt(3)': '3.05080%', 'hmdh(4)': '6.04587%', 'akt1(5)': '20.28843%', 'ada17(6)': '57.87501%'}\n",
      "  Highest Probability: {'ada17(6)': '57.9%'}\n",
      "\n",
      "Sample # 1\n",
      "  Labeled ProbDist %: {'fabp4(0)': '19.31451%', 'kith(1)': '17.74733%', 'cxcr4(2)': '17.84510%', 'comt(3)': '22.97921%', 'hmdh(4)': '12.81689%', 'akt1(5)': '5.96058%', 'ada17(6)': '3.33638%'}\n",
      "  Highest Probability: {'comt(3)': '23.0%'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TIP: You can inspect a pth file with https://netron.app/ online.\n",
    "\n",
    "\n",
    "def plot_predictions(maps, label_mapping, smiles=None, lab=None, path=None):\n",
    "    \"\"\"\n",
    "    Plot the probability distribution for each sample.\n",
    "    The highest probability is highlighted with a dotted outline and a horizontal dotted line.\n",
    "    \n",
    "    Parameters:\n",
    "      maps: np.array of model probabilities (N x num_classes)\n",
    "      label_mapping: dict mapping labels to indices\n",
    "      smiles (optional): np.array of SMILES strings for each sample\n",
    "      lab (optional): np.array of ground-truth labels for each sample\n",
    "    \"\"\"\n",
    "    num_samples = len(maps)\n",
    "    # Create a list of label names in the format \"label(index)\"\n",
    "    labels = [f\"{label}({idx})\" for label, idx in label_mapping.items()]\n",
    "    x = np.arange(len(labels))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        probs = maps[i]\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        \n",
    "        # Plot the bars (convert probabilities to percentage)\n",
    "        bars = plt.bar(x, probs * 100, align='center', alpha=0.7)\n",
    "        \n",
    "        # Determine the maximum probability index and value\n",
    "        max_index = probs.argmax()\n",
    "        max_prob_percent = probs[max_index] * 100\n",
    "        \n",
    "        # Highlight the highest probability bar:\n",
    "        # - Set a red dotted outline on the highest bar\n",
    "        bars[max_index].set_edgecolor('red')\n",
    "        bars[max_index].set_linewidth(2)\n",
    "        bars[max_index].set_linestyle('--')\n",
    "        \n",
    "        # Draw a horizontal dotted line at the highest probability level\n",
    "        plt.axhline(y=max_prob_percent, color='red', linestyle='--', linewidth=1)\n",
    "        \n",
    "        # Annotate the highest probability value above the corresponding bar\n",
    "        plt.text(x[max_index], max_prob_percent + 1, f\"{max_prob_percent:.1f}%\", \n",
    "                 ha='center', color='red', fontweight='bold')\n",
    "        \n",
    "        # Set the x-axis tick labels\n",
    "        plt.xticks(x, labels, rotation=45)\n",
    "        plt.xlabel('Labels')\n",
    "        plt.ylabel('Probability (%)')\n",
    "        \n",
    "        # Use SMILES as title if provided; otherwise, just sample number\n",
    "        title_str = f\"Sample #{i}\"\n",
    "        if smiles is not None:\n",
    "            title_str += f\": {smiles[i]}\"\n",
    "        plt.title(title_str)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig(f\"{path}/prediction_{i}.png\")\n",
    "\n",
    "def display_predictions(maps, label_mapping, smiles=None, lab=None):\n",
    "    \"\"\"\n",
    "    Display the probability distribution for each sample.\n",
    "    \n",
    "    Parameters:\n",
    "      maps: np.array of model probabilities (N x num_classes)\n",
    "      label_mapping: dict mapping labels to indices\n",
    "      smiles (optional): np.array of SMILES strings for each sample\n",
    "      lab (optional): np.array of ground-truth labels for each sample\n",
    "    \"\"\"\n",
    "    for i in range(len(maps)):\n",
    "        print(\"Sample #\", i)\n",
    "        if smiles is not None:\n",
    "            print(\"  SMILES:   \", smiles[i])\n",
    "            \n",
    "        # Convert each probability to a percentage string with 5 decimal places\n",
    "        percentage_probs = [f\"{p * 100:.5f}%\" for p in maps[i]]\n",
    "        \n",
    "        # Create a dictionary mapping each label (with index) to its corresponding probability percentage\n",
    "        labeled_percentage_probs = {\n",
    "            f\"{label}({idx})\": percentage_probs[idx] \n",
    "            for label, idx in label_mapping.items()\n",
    "        }\n",
    "        \n",
    "        # Identify the index of the highest probability for this sample\n",
    "        max_index = maps[i].argmax()\n",
    "        highest_prob = maps[i][max_index]\n",
    "        \n",
    "        # Get the corresponding label for the highest probability, with a default if not found\n",
    "        highest_label = next((label for label, idx in label_mapping.items() if idx == max_index), f\"Index {max_index}\")\n",
    "        \n",
    "        # Create a dictionary for the highest probability using the \"label(index)\" format\n",
    "        labeled_percentage_highest_probs = {\n",
    "            f\"{highest_label}({max_index})\": f\"{highest_prob * 100:.1f}%\"\n",
    "        }\n",
    "        \n",
    "        print(\"  Labeled ProbDist %:\", labeled_percentage_probs)\n",
    "        print(\"  Highest Probability:\", labeled_percentage_highest_probs)\n",
    "        print()  # Empty line for readability\n",
    "\n",
    "# --- Example usage with the fully trained model branch ---\n",
    "use_fully_trained_model = False\n",
    "\n",
    "if use_fully_trained_model:\n",
    "    checkpoint = torch.load(os.path.join(\"PharmaNet/Best_Config/model1\", 'test_predictions.pth'))\n",
    "    \n",
    "    \n",
    "    # here we get the original label mapping\n",
    "    df = pd.read_csv('data/datasets/DUDE/Smiles_1.csv')\n",
    "    unique_targets = df.drop_duplicates(subset='Target')\n",
    "    label_mapping = dict(zip(unique_targets['Target'], unique_targets['Label']))\n",
    "    \n",
    "    # Extract the 'prediction' entry which contains [smiles, maps, lab]\n",
    "    predictions_map = checkpoint['prediction']\n",
    "    smiles = predictions_map[0]  # np.array of SMILES strings\n",
    "    maps   = predictions_map[1]  # np.array of model probabilities (N x num_classes)\n",
    "    lab    = predictions_map[2]  # np.array of ground-truth class labels\n",
    "\n",
    "    print(\"SMILES array shape:\", smiles.shape)\n",
    "    print(\"Probability array shape:\", maps.shape)\n",
    "    print(\"Labels array shape:\", lab.shape)\n",
    "    \n",
    "    # Use the reusable function to display predictions for each sample\n",
    "    display_predictions(maps, label_mapping, smiles=smiles, lab=lab)\n",
    "    # plot_predictions(maps, label_mapping, path=\"PharmaNet/Best_Config/model1\")\n",
    "    \n",
    "else:\n",
    "    # Use an alternative checkpoint path\n",
    "    label_mapping = {'fabp4': 0, 'kith': 1, 'cxcr4': 2, 'comt': 3, 'hmdh': 4, 'akt1': 5, 'ada17': 6}\n",
    "    checkpoint = torch.load(os.path.join(saving_path_models, 'test_predictions.pth'))\n",
    "    \n",
    "    # Extract the 'prediction' entry (here, we assume it returns a dict with key 'outputs')\n",
    "    predictions_map = checkpoint['prediction']\n",
    "    maps = predictions_map['outputs']  # np.array of model probabilities (N x num_classes)\n",
    "    \n",
    "    print(\"Probability array shape:\", maps.shape)\n",
    "    \n",
    "    # Use the reusable function without SMILES and ground-truth labels\n",
    "    display_predictions(maps, label_mapping)\n",
    "    plot_predictions(maps, label_mapping, path=saving_path_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PharmaNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
